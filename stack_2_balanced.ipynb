{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler,KBinsDiscretizer,LabelEncoder,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,f1_score\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from catboost import Pool, cv,CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate,Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df_final_blanced.csv')\n",
    "train_df = train_df.fillna(0)\n",
    "train_df_org = pd.read_csv('train_df_final.csv')\n",
    "train_df_org = train_df_org.fillna(0)\n",
    "test_df = pd.read_csv('test_df_final.csv')\n",
    "test_df = test_df.fillna(0)\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['label'].values\n",
    "y_org = train_df_org['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum()/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions_train = pd.DataFrame()\n",
    "model_predictions_train['label'] = train_df['label']\n",
    "model_predictions_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'fare_anomaly',\n",
    "    'additional_fare_anomaly', \n",
    "    'duration_anomaly',\n",
    "    'meter_waiting_anomaly', \n",
    "    'meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_till_pickup_anomaly', \n",
    "    'additional_fare_duration_anomaly',\n",
    "    'additional_fare_meter_waiting_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_anomaly', \n",
    "    'duration_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    'fare_anomaly',\n",
    "    'additional_fare_anomaly', \n",
    "    'duration_anomaly',\n",
    "    'meter_waiting_anomaly', \n",
    "    'meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_till_pickup_anomaly', \n",
    "    'additional_fare_duration_anomaly',\n",
    "    'additional_fare_meter_waiting_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_anomaly', \n",
    "    'duration_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "train_org = train_df_org[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)\n",
    "    train_org[each] = train_org[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat,average='micro'), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)\n",
    "org_pool = Pool(data=train_org[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class = model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(org_pool),average='micro'))\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores =[]\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,np.round(model.predict(train_org)),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':15,\n",
    "    'weights':'uniform'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators':50,\n",
    "    'max_depth':10,\n",
    "    'random_state':0,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'rf_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':2,\n",
    "    'kernel':'linear',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svc_linear_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':2,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svc_rbf_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'additional_fare',\n",
    "    'duration',\n",
    "    'meter_waiting',\n",
    "    'meter_waiting_fare',\n",
    "    'meter_waiting_till_pickup',\n",
    "    'fare',\n",
    "    'pickup_date',\n",
    "    'pickup_hour',\n",
    "    'pickup_minute',\n",
    "    'drop_date',\n",
    "    'drop_hour',\n",
    "    'drop_minute',\n",
    "    'pick_cluster',\n",
    "    'is_more_than_one_day',\n",
    "    'distance_km',\n",
    "    'fare_per_km',\n",
    "    'pickup_timeslot',\n",
    "    'day_of_week',\n",
    "    'is_weekday',\n",
    "    'cal_time_difference',\n",
    "    'fare_per_distance',\n",
    "    'fare_per_duration',\n",
    "    'avg_speed',\n",
    "    'meter_waiting_per_duration',\n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_fare_per_duration',\n",
    "    'addtional_fare_per_fare',\n",
    "    'addtional_fare_per_distance',\n",
    "    'addtional_fare_per_duration',\n",
    "    'fare-additional_fare',\n",
    "    'fare-additional_fare-meter_waiting_fare',\n",
    "    'fare-additional_fare_per_distance',\n",
    "    'fare-additional_fare_per_duration',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_distance',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_duration',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting',\n",
    "    'meter_waiting_after_pickup',\n",
    "    'meter_waiting_after_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_distance',\n",
    "    'meter_waiting_after_pickup_per_distance',\n",
    "    'meter_waiting_till_pickup_per_fare',\n",
    "    'meter_waiting_after_pickup_per_fare',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting_fare',\n",
    "    'meter_waiting_after_pickup_per_meter_waiting_fare',\n",
    "    'fare_per_distance_mean',\n",
    "    'fare_per_distance_mean_diff',\n",
    "    'avg_speed_mean',\n",
    "    'avg_speed_mean_diff',\n",
    "    'meter_waiting_per_duration_mean',\n",
    "    'meter_waiting_per_duration_mean_diff',\n",
    "    'meter_waiting_fare_per_meter_waiting_mean',\n",
    "    'meter_waiting_fare_per_meter_waiting_mean_diff',\n",
    "    'meter_waiting_fare_per_duration_mean',\n",
    "    'meter_waiting_fare_per_duration_mean_diff',\n",
    "    'addtional_fare_per_fare_mean',\n",
    "    'addtional_fare_per_fare_mean_diff',\n",
    "    'addtional_fare_per_distance_mean',\n",
    "    'addtional_fare_per_distance_mean_diff',\n",
    "    'addtional_fare_per_duration_mean',\n",
    "    'addtional_fare_per_duration_mean_diff',\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    'pickup_date',\n",
    "    'pickup_hour',\n",
    "    'pickup_minute',\n",
    "    'drop_date',\n",
    "    'drop_hour',\n",
    "    'drop_minute',\n",
    "    'pick_cluster',\n",
    "    'is_more_than_one_day',\n",
    "    'pickup_timeslot',\n",
    "    'day_of_week',\n",
    "    'is_weekday',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "train_org = train_df_org[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)\n",
    "    train_org[each] = train_org[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)\n",
    "org_pool = Pool(data=train_org[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class = model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(org_pool),average='micro'))\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_base'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,np.round(model.predict(train_org)),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_base'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear prediction difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'predicted_fare_diff',\n",
    "    'predicted_fare_diff_per_fare',\n",
    "    'predicted_fare_diff_per_predicted_fare',\n",
    "    'predicted_fare_diff_per_distance',\n",
    "    'predicted_duration_diff',\n",
    "    'predicted_duraton_diff_per_duraton',\n",
    "    'predicted_duraton_diff_per_predicted_duration',\n",
    "    'predicted_duraton_diff_per_distance',\n",
    "    'predicted_fare_per_duration_diff',\n",
    "    'predicted_avg_speed_diff',\n",
    "    'predicted_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting',\n",
    "    'predicted_meter_waiting_diff_per_distance',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting',\n",
    "    'predicted_meter_waiting_per_duration_diff',\n",
    "    'predicted_meter_waiting_fare_diff',\n",
    "    'predicted_meter_waiting_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff',\n",
    "    'predicted_additional_fare_diff',\n",
    "    'predicted_additional_fare_diff_per_additional_fare',\n",
    "    'predicted_addtional_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_till_pickup_diff',\n",
    "    'predicted_meter_waiting_till_pickup_diff_per_meter_waiting_till_pickup',\n",
    "    'predicted_meter_waiting_till_pickup_per_meter_waiting_diff',\n",
    "    'predicted_fare_diff_per_distance_normalized',\n",
    "    'predicted_fare_diff_normalized',\n",
    "    'predicted_fare_diff_per_fare_normalized',\n",
    "    'predicted_fare_diff_per_predicted_fare_normalized',\n",
    "    'predicted_duraton_diff_per_duraton_normalized',\n",
    "    'predicted_duraton_diff_per_predicted_duration_normalized',\n",
    "    'predicted_fare_per_duration_diff_normalized',\n",
    "    'predicted_avg_speed_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_per_duration_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff_normalized',\n",
    "    'predicted_additional_fare_diff_normalized',\n",
    "    'predicted_additional_fare_diff_per_additional_fare_normalized',\n",
    "]\n",
    "\n",
    "cat_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "train_org = train_df_org[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)\n",
    "    train_org[each] = train_org[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)\n",
    "org_pool = Pool(data=train_org[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class = model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(org_pool),average='micro'))\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_pred_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,np.round(model.predict(train_org)),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_pred_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unit differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'predicted_fare_diff_per_fare',\n",
    "    'predicted_fare_diff_per_predicted_fare',\n",
    "    'predicted_fare_diff_per_distance',\n",
    "    'predicted_duraton_diff_per_duraton',\n",
    "    'predicted_duraton_diff_per_predicted_duration',\n",
    "    'predicted_duraton_diff_per_distance',\n",
    "    'predicted_fare_per_duration_diff',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting',\n",
    "    'predicted_meter_waiting_diff_per_distance',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting',\n",
    "    'predicted_meter_waiting_per_duration_diff',\n",
    "    'predicted_meter_waiting_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff',\n",
    "    'predicted_additional_fare_diff_per_additional_fare',\n",
    "    'predicted_addtional_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_till_pickup_diff_per_meter_waiting_till_pickup',\n",
    "    'predicted_meter_waiting_till_pickup_per_meter_waiting_diff'\n",
    "]\n",
    "\n",
    "cat_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "train_org = train_df_org[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)\n",
    "    train_org[each] = train_org[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':10,\n",
    "    'weights':'distance'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_unit_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':5,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'auto'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'rbf_svm_unit_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diff norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'predicted_fare_diff_per_distance_normalized',\n",
    "    'predicted_fare_diff_normalized',\n",
    "    'predicted_fare_diff_per_fare_normalized',\n",
    "    'predicted_fare_diff_per_predicted_fare_normalized',\n",
    "    'predicted_duraton_diff_per_duraton_normalized',\n",
    "    'predicted_duraton_diff_per_predicted_duration_normalized',\n",
    "    'predicted_fare_per_duration_diff_normalized',\n",
    "    'predicted_avg_speed_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_per_duration_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff_normalized',\n",
    "    'predicted_additional_fare_diff_normalized',\n",
    "    'predicted_additional_fare_diff_per_additional_fare_normalized',\n",
    "]\n",
    "\n",
    "cat_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "train_org = train_df_org[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)\n",
    "    train_org[each] = train_org[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':15,\n",
    "    'weights':'distance'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_unit_diff_norm'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':5,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(org_scores), np.std(org_scores), min(org_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svm_rbf_unit_diff_norm'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions_train.to_csv('stack_2_balanced_train.csv',index=False)\n",
    "model_predictions_test.to_csv('stack_2_balanced_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model_predictions_test.columns\n",
    "cat_cols = [\n",
    "    'knn_anomaly',\n",
    "    'rf_anomaly',\n",
    "    'knn_unit_diff',\n",
    "    'svc_linear_anomaly',\n",
    "    'svc_rbf_anomaly',\n",
    "    'knn_unit_diff_norm',\n",
    "    'rbf_svm_unit_diff',\n",
    "    'knn_unit_diff_norm',\n",
    "    'svm_rbf_unit_diff_norm'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in cat_cols:\n",
    "    model_predictions_train[each] = model_predictions_train[each].astype(int)\n",
    "    model_predictions_test[each] = model_predictions_test[each].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model_predictions_train[features]\n",
    "test = model_predictions_test[features]\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(model_predictions_train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=model_predictions_test[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class = model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extratrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {\n",
    "    'n_estimators':10,\n",
    "    'max_depth':3,\n",
    "    'random_state':0    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = ExtraTreesClassifier(**et_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(validation_scores), np.std(validation_scores), min(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
