{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import geopy.distance\n",
    "import itertools\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import SVR,SVC\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest,ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler,KBinsDiscretizer,LabelEncoder,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,f1_score,confusion_matrix,log_loss\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA,NMF\n",
    "\n",
    "from catboost import Pool, cv,CatBoostClassifier,CatBoostRegressor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate,Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier,XGBRegressor,DMatrix,plot_tree\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "## Shortened eda notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time(feature='date'):\n",
    "    def f(time_stamp): \n",
    "        date,time = time_stamp.strip().split()\n",
    "        date = list(map(int, date.split('/')))\n",
    "        time = list(map(int, time.split(':')))\n",
    "        if feature == 'date':\n",
    "            return date[1]\n",
    "        if feature == 'month':\n",
    "            return date[0]\n",
    "        if feature == 'year':\n",
    "            return date[2]\n",
    "        if feature == 'hour':\n",
    "            return time[0]\n",
    "        if feature == 'minute':\n",
    "            return time[1]\n",
    "    return f\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_dataset(cols, train=train_df, test=test_df):\n",
    "    tmp_1 = train.loc[:,cols]\n",
    "    tmp_1['dataset'] = 'train'\n",
    "\n",
    "    tmp_2 = test.loc[:,cols]\n",
    "    tmp_2['dataset'] = 'test'\n",
    "    return tmp_1.append(tmp_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df.dropna().drop(['tripid'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pickup_month'] = data['pickup_time'].map(extract_time('month'))\n",
    "data['pickup_date'] = data['pickup_time'].map(extract_time('date'))\n",
    "data['pickup_year'] = data['pickup_time'].map(extract_time('year'))\n",
    "data['pickup_hour'] = data['pickup_time'].map(extract_time('hour'))\n",
    "data['pickup_minute'] = data['pickup_time'].map(extract_time('minute'))\n",
    "\n",
    "data['drop_month'] = data['drop_time'].map(extract_time('month'))\n",
    "data['drop_date'] = data['drop_time'].map(extract_time('date'))\n",
    "data['drop_year'] = data['drop_time'].map(extract_time('year'))\n",
    "data['drop_hour'] = data['drop_time'].map(extract_time('hour'))\n",
    "data['drop_minute'] = data['drop_time'].map(extract_time('minute'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pickup_month'] = test_df['pickup_time'].map(extract_time('month'))\n",
    "test_df['pickup_date'] = test_df['pickup_time'].map(extract_time('date'))\n",
    "test_df['pickup_year'] = test_df['pickup_time'].map(extract_time('year'))\n",
    "test_df['pickup_hour'] = test_df['pickup_time'].map(extract_time('hour'))\n",
    "test_df['pickup_minute'] = test_df['pickup_time'].map(extract_time('minute'))\n",
    "\n",
    "test_df['drop_month'] = test_df['drop_time'].map(extract_time('month'))\n",
    "test_df['drop_date'] = test_df['drop_time'].map(extract_time('date'))\n",
    "test_df['drop_year'] = test_df['drop_time'].map(extract_time('year'))\n",
    "test_df['drop_hour'] = test_df['drop_time'].map(extract_time('hour'))\n",
    "test_df['drop_minute'] = test_df['drop_time'].map(extract_time('minute'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['drop_lat'] < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_more_than_one_day(row):\n",
    "    return 1 if row['pickup_date'] != row['drop_date'] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_more_than_one_day'] = data.apply(is_more_than_one_day,axis=1)\n",
    "test_df['is_more_than_one_day'] = test_df.apply(is_more_than_one_day,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['pick_lat','pick_lon']\n",
    "comb_data = get_combined_dataset(cols, train=data)\n",
    "\n",
    "gmm_pick = BayesianGaussianMixture(n_components=3)\n",
    "gmm_pick.fit(comb_data[cols].values)\n",
    "\n",
    "data['pick_cluster'] = gmm_pick.predict(data[cols].values)\n",
    "test_df['pick_cluster'] = gmm_pick.predict(test_df[cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['drop_lat','drop_lon']\n",
    "comb_data = get_combined_dataset(cols, train=data)\n",
    "\n",
    "gmm_drop = BayesianGaussianMixture(n_components=3,max_iter=1000)\n",
    "gmm_drop.fit(comb_data[cols].values)\n",
    "\n",
    "data['drop_cluster'] = gmm_drop.predict(data[cols].values)\n",
    "test_df['drop_cluster'] = gmm_drop.predict(test_df[cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trip_distance(row):\n",
    "    coords_1 = (row['pick_lat'],row['pick_lon'])\n",
    "    coords_2 = (row['drop_lat'],row['drop_lon'])\n",
    "    return geopy.distance.geodesic(coords_1, coords_2).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['distance_km'] = data.apply(calculate_trip_distance,axis=1).clip(0,100)\n",
    "test_df['distance_km'] = test_df.apply(calculate_trip_distance,axis=1).clip(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fare_per_distance(row):\n",
    "    return row['fare'] / (row['distance_km']+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fare_per_km'] = data.apply(fare_per_distance,axis=1)\n",
    "test_df['fare_per_km'] = test_df.apply(fare_per_distance,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_slot(row,by='pickup'):\n",
    "    hour = row[f'{by}_hour']\n",
    "    if 7 <= hour <= 9:\n",
    "        return 1\n",
    "    if 12 <= hour <= 2:\n",
    "        return 2\n",
    "    if 4 <= hour <= 6:\n",
    "        return 3\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pickup_timeslot'] = data.apply(time_slot,axis=1)\n",
    "test_df['pickup_timeslot'] = test_df.apply(time_slot,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_week(row,by='pickup'):\n",
    "    date = row[f'{by}_date']\n",
    "    month = row[f'{by}_month']\n",
    "    year = row[f'{by}_year']\n",
    "    d = datetime.datetime(year,month,date).weekday()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day_of_week'] = data.apply(day_of_week,axis=1)\n",
    "test_df['day_of_week'] = test_df.apply(day_of_week,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekday(row,by='pickup'):\n",
    "    date = row['day_of_week']\n",
    "    return 1 if date < 5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_weekday'] = data.apply(is_weekday,axis=1)\n",
    "test_df['is_weekday'] = test_df.apply(is_weekday,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_time_difference(row):\n",
    "    pickup_date = row['pickup_date']\n",
    "    pickup_month = row['pickup_month']\n",
    "    pickup_year = row['pickup_year']\n",
    "    pickup_hour = row['pickup_hour']\n",
    "    pickup_minute = row['pickup_minute']\n",
    "    pickup_time = datetime.datetime(pickup_year, pickup_month, pickup_date, pickup_hour, pickup_minute)\n",
    "    \n",
    "    drop_date = row['drop_date']\n",
    "    drop_month = row['drop_month']\n",
    "    drop_year = row['drop_year']\n",
    "    drop_hour = row['drop_hour']\n",
    "    drop_minute = row['drop_minute']\n",
    "    drop_time = datetime.datetime(drop_year, drop_month, drop_date, drop_hour, drop_minute)\n",
    "    \n",
    "    delta = drop_time - pickup_time\n",
    "    return delta.seconds - row['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cal_time_difference'] = data.apply(cal_time_difference,axis=1)\n",
    "test_df['cal_time_difference'] = test_df.apply(cal_time_difference,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    if label == 'correct':\n",
    "        return 1\n",
    "    elif label == 'incorrect':\n",
    "        return 0\n",
    "    else:\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['label'].map(encode_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_save = [\n",
    "    'additional_fare', \n",
    "    'duration', \n",
    "    'meter_waiting', \n",
    "    'meter_waiting_fare',\n",
    "    'meter_waiting_till_pickup', \n",
    "    'fare',\n",
    "    'pickup_date', \n",
    "    'pickup_hour', \n",
    "    'pickup_minute',\n",
    "    'drop_date', \n",
    "    'drop_hour', \n",
    "    'drop_minute',\n",
    "    'pick_cluster',\n",
    "    'is_more_than_one_day',\n",
    "    'distance_km',\n",
    "    'fare_per_km',\n",
    "    'pickup_timeslot',\n",
    "    'day_of_week',\n",
    "    'is_weekday',\n",
    "    'cal_time_difference']\n",
    "data.loc[:, columns_to_save+['label']].to_csv('train_df.csv',index=False)\n",
    "test_df.loc[:, columns_to_save].to_csv('test_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortened anomaly_detection notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "data = train_df[train_df['label'] == 1].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_pred(col, train_df=train_df, test_df=test_df, folds=3):\n",
    "    labels = train_df['label'].values\n",
    "    X = train_df[col].values\n",
    "\n",
    "    X_train_df = train_df[col].values\n",
    "    X_test_df = test_df[col].values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "    validation_scores = []\n",
    "    models = []\n",
    "\n",
    "    train_preds = np.zeros(train_df.shape[0])\n",
    "    test_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    for train_index, test_index in skf.split(X, labels):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        X_train = X_train.reshape((-1,1))\n",
    "        X_test = X_test.reshape((-1,1))\n",
    "\n",
    "        model = IsolationForest(random_state=0).fit(X_train)\n",
    "        preds = model.predict(X_test).clip(0,1).reshape(y_test.shape)\n",
    "        validation_score = f1_score(y_test, preds)\n",
    "\n",
    "        train_preds += model.predict(X_train_df.reshape(-1,1)).reshape(X_train_df.shape).clip(0,1)\n",
    "        test_preds += model.predict(X_test_df.reshape(-1,1)).reshape(X_test_df.shape).clip(0,1)\n",
    "\n",
    "    #     print('Validation score:' , validation_score)\n",
    "\n",
    "        validation_scores.append(validation_score)\n",
    "        models.append(model)\n",
    "        \n",
    "    train_df[f'{col}_anomaly'] = np.where(train_preds > 2, 1, 0)\n",
    "    test_df[f'{col}_anomaly'] = np.where(test_preds > 2, 1, 0)\n",
    "    return validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fare','additional_fare','duration','meter_waiting','meter_waiting_fare','meter_waiting_till_pickup']\n",
    "for col in tqdm(cols):\n",
    "    validation_scores = anomaly_pred(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_pred_multi(cols, train_df=train_df, test_df=test_df, folds=3):\n",
    "    labels = train_df['label'].values\n",
    "    X = train_df[cols].values\n",
    "\n",
    "    X_train_df = train_df[cols].values\n",
    "    X_test_df = test_df[cols].values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "    validation_scores = []\n",
    "    models = []\n",
    "\n",
    "    train_preds = np.zeros(train_df.shape[0])\n",
    "    test_preds = np.zeros(test_df.shape[0])\n",
    "#     print(X.shape)\n",
    "\n",
    "    for train_index, test_index in skf.split(X, labels):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        model = IsolationForest(random_state=0).fit(X_train)\n",
    "        preds = model.predict(X_test).clip(0,1)\n",
    "        validation_score = f1_score(y_test, preds)\n",
    "\n",
    "        train_preds += model.predict(X_train_df).clip(0,1)\n",
    "        test_preds += model.predict(X_test_df).clip(0,1)\n",
    "\n",
    "    #     print('Validation score:' , validation_score)\n",
    "\n",
    "        validation_scores.append(validation_score)\n",
    "        models.append(model)\n",
    "    name = '_'.join(cols)\n",
    "    train_df[f'{name}_anomaly'] = np.where(train_preds > 2, 1, 0)\n",
    "    test_df[f'{name}_anomaly'] = np.where(test_preds > 2, 1, 0)\n",
    "    return validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fare','additional_fare','duration','meter_waiting','meter_waiting_fare','meter_waiting_till_pickup']\n",
    "for i, col_1 in enumerate(cols):\n",
    "    for col_2 in cols[i+1:]:\n",
    "        validation_scores = anomaly_pred_multi([col_1,col_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col_1 in enumerate(cols):\n",
    "    for col_2 in cols[i+1:]:\n",
    "        j = cols.index(col_2)\n",
    "        for col_3 in cols[j+1:]:\n",
    "            validation_scores = anomaly_pred_multi([col_1,col_2,col_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col_1 in enumerate(cols):\n",
    "    for col_2 in cols[i+1:]:\n",
    "        j = cols.index(col_2)\n",
    "        for col_3 in cols[j+1:]:\n",
    "            k = cols.index(col_3)\n",
    "            for col_4 in cols[k+1:]:                \n",
    "                validation_scores = anomaly_pred_multi([col_1,col_2,col_3,col_4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "#     'class_weights':class_weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'fare_anomaly',\n",
    "    'additional_fare_anomaly', \n",
    "    'duration_anomaly',\n",
    "    'meter_waiting_anomaly', \n",
    "    'meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_till_pickup_anomaly', \n",
    "    'additional_fare_duration_anomaly',\n",
    "    'additional_fare_meter_waiting_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_anomaly', \n",
    "    'duration_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    \n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    'additional_fare_anomaly', \n",
    "    'duration_anomaly',\n",
    "    'meter_waiting_anomaly', \n",
    "    'meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_till_pickup_anomaly', \n",
    "    'additional_fare_duration_anomaly',\n",
    "    'additional_fare_meter_waiting_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_anomaly', \n",
    "    'duration_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df['label'].values\n",
    "train_df = train_df.drop(['label'], axis=1)[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)\n",
    "train_df_pool = Pool(data=train_df[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "submission_preds = np.zeros(submission_df.shape[0])\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_pools = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train_df, labels):\n",
    "    X_train, X_test = train_df.iloc[train_index,:], train_df.iloc[test_index,:]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    pred = model.predict(test_pool)\n",
    "    validation_score = model.best_score_['validation']['F1']\n",
    "    print('Validation f1',validation_score)\n",
    "    validation_scores.append(validation_score)\n",
    "    models.append(model)\n",
    "    train_pools.append(train_pool)\n",
    "    submission_preds += model.predict(submission_pool)\n",
    "    train_preds += model.predict_proba(train_df_pool)[:,1]\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df_anomaly.csv',index=False)\n",
    "test_df.to_csv('test_df_anomaly.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortened noise_pre_eda notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "data = train_df[train_df['label'] == 1].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['duration','meter_waiting','meter_waiting_fare','is_more_than_one_day']\n",
    "X = data[cols].values\n",
    "y = data['fare'].values\n",
    "\n",
    "X_train_df = train_df[cols].values\n",
    "X_test_df = test_df[cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_df = scaler.transform(X_train_df)\n",
    "X_test_df = scaler.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "validation_scores = []\n",
    "models = []\n",
    "\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "kf = KFold(n_splits=folds)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    \n",
    "#     model = CatBoostRegressor(**params)\n",
    "#     model.fit(X=X_train,y=y_train,eval_set=(X_test,y_test))\n",
    "\n",
    "    model = LinearRegression()\n",
    "#     model = SVR()\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test,pred) ** 0.5\n",
    "    validation_scores.append(score)\n",
    "    models.append(model)\n",
    "    print('RMSE:', score)\n",
    "    \n",
    "    train_preds += model.predict(X_train_df)\n",
    "    test_preds += model.predict(X_test_df)\n",
    "    \n",
    "train_preds /= folds\n",
    "test_preds /= folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_fare'] = train_preds\n",
    "test_df['predicted_fare'] = test_preds\n",
    "\n",
    "train_df['predicted_fare_diff'] = train_df['fare'] - train_df['predicted_fare']\n",
    "test_df['predicted_fare_diff'] = test_df['fare'] - test_df['predicted_fare']    \n",
    "\n",
    "train_df['predicted_fare_diff_per_fare'] = train_df['predicted_fare_diff'] / (train_df['fare']+1)\n",
    "test_df['predicted_fare_diff_per_fare'] = test_df['predicted_fare_diff'] / (test_df['fare']+1)\n",
    "\n",
    "train_df['predicted_fare_diff_per_predicted_fare'] = train_df['predicted_fare_diff'] / (train_df['predicted_fare']+1)\n",
    "test_df['predicted_fare_diff_per_predicted_fare'] = test_df['predicted_fare_diff'] / (test_df['predicted_fare']+1)\n",
    "\n",
    "train_df['fare_per_distance'] = train_df['fare'] / (train_df['distance_km']+1)\n",
    "test_df['fare_per_distance'] = test_df['fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_fare_per_distance'] = train_df['predicted_fare'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_fare_per_distance'] = test_df['predicted_fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_fare_diff_per_distance'] = train_df['predicted_fare_diff'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_fare_diff_per_distance'] = test_df['predicted_fare_diff'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_fare_diff_per_fare'] = train_df['predicted_fare_diff'] / (train_df['fare']+1)\n",
    "test_df['predicted_fare_diff_per_fare'] = test_df['predicted_fare_diff'] / (test_df['fare']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['meter_waiting','meter_waiting_fare','fare','is_more_than_one_day','cal_time_difference']\n",
    "\n",
    "X = data[cols].values\n",
    "y = data['duration'].values\n",
    "\n",
    "X_train_df = train_df[cols].values\n",
    "X_test_df = test_df[cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_df = scaler.transform(X_train_df)\n",
    "X_test_df = scaler.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "\n",
    "validation_scores = []\n",
    "models = []\n",
    "\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "kf = KFold(n_splits=folds)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "   \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test,pred) ** 0.5\n",
    "    validation_scores.append(score)\n",
    "    models.append(model)\n",
    "    print('RMSE:', score)\n",
    "    \n",
    "    train_preds += model.predict(X_train_df)\n",
    "    test_preds += model.predict(X_test_df)\n",
    "    \n",
    "train_preds /= folds\n",
    "test_preds /= folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_duration'] = train_preds\n",
    "test_df['predicted_duration'] = test_preds\n",
    "\n",
    "train_df['predicted_duration_diff'] = train_df['duration'] - train_df['predicted_duration']\n",
    "test_df['predicted_duration_diff'] = test_df['duration'] - test_df['predicted_duration']    \n",
    "\n",
    "train_df['predicted_duraton_diff_per_duraton'] = train_df['predicted_duration_diff'] / (train_df['duration']+1)\n",
    "test_df['predicted_duraton_diff_per_duraton'] = test_df['predicted_duration_diff'] / (test_df['duration']+1)\n",
    "\n",
    "train_df['predicted_duraton_diff_per_predicted_duration'] = train_df['predicted_duration_diff'] / (train_df['predicted_duration']+1)\n",
    "test_df['predicted_duraton_diff_per_predicted_duration'] = test_df['predicted_duration_diff'] / (test_df['predicted_duration']+1)\n",
    "\n",
    "train_df['predicted_duraton_diff_per_distance'] = train_df['predicted_duration_diff'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_duraton_diff_per_distance'] = test_df['predicted_duration_diff'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['fare_per_duration'] = train_df['fare'] / (train_df['duration']+1)\n",
    "test_df['fare_per_duration'] = test_df['fare'] / (test_df['duration']+1)\n",
    "\n",
    "train_df['predicted_fare_per_duration'] = train_df['predicted_fare'] / (train_df['predicted_duration']+1)\n",
    "test_df['predicted_fare_per_duration'] = test_df['predicted_fare'] / (test_df['predicted_duration']+1)\n",
    "\n",
    "train_df['predicted_fare_per_duration_diff'] = train_df['fare_per_duration'] - train_df['predicted_fare_per_duration']\n",
    "test_df['predicted_fare_per_duration_diff'] = test_df['fare_per_duration'] - test_df['predicted_fare_per_duration']\n",
    "\n",
    "train_df['avg_speed'] = train_df['distance_km'] / (train_df['duration'] + 1)\n",
    "test_df['avg_speed'] = test_df['distance_km'] / (test_df['duration'] + 1)\n",
    "\n",
    "train_df['predicted_avg_speed'] = train_df['distance_km'] / (train_df['predicted_duration'] + 1)\n",
    "test_df['predicted_avg_speed'] = test_df['distance_km'] / (test_df['predicted_duration'] + 1)\n",
    "\n",
    "train_df['predicted_avg_speed_diff'] = train_df['avg_speed'] - train_df['predicted_avg_speed']\n",
    "test_df['predicted_avg_speed_diff'] = test_df['avg_speed'] - test_df['predicted_avg_speed']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['duration','meter_waiting_fare','fare','cal_time_difference']\n",
    "\n",
    "X = data[cols].values\n",
    "y = data['meter_waiting'].values\n",
    "\n",
    "X_train_df = train_df[cols].values\n",
    "X_test_df = test_df[cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_df = scaler.transform(X_train_df)\n",
    "X_test_df = scaler.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "\n",
    "validation_scores = []\n",
    "models = []\n",
    "\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "kf = KFold(n_splits=folds)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test,pred) ** 0.5\n",
    "    validation_scores.append(score)\n",
    "    models.append(model)\n",
    "    print('RMSE:', score)\n",
    "    \n",
    "    train_preds += model.predict(X_train_df)\n",
    "    test_preds += model.predict(X_test_df)\n",
    "    \n",
    "train_preds /= folds\n",
    "test_preds /= folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_meter_waiting'] = train_preds\n",
    "test_df['predicted_meter_waiting'] = test_preds\n",
    "\n",
    "train_df['predicted_meter_waiting_diff'] = train_df['meter_waiting'] - train_df['predicted_meter_waiting']\n",
    "test_df['predicted_meter_waiting_diff'] = test_df['meter_waiting'] - test_df['predicted_meter_waiting']\n",
    "\n",
    "train_df['predicted_meter_waiting_diff_per_meter_waiting'] = train_df['predicted_meter_waiting_diff'] / (train_df['meter_waiting'] + 1)\n",
    "test_df['predicted_meter_waiting_diff_per_meter_waiting'] = test_df['predicted_meter_waiting_diff'] / (test_df['meter_waiting'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_diff_per_distance'] = train_df['predicted_meter_waiting_diff'] / (train_df['distance_km'] + 1)\n",
    "test_df['predicted_meter_waiting_diff_per_distance'] = test_df['predicted_meter_waiting_diff'] / (test_df['distance_km'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_diff_per_predicted_meter_waiting'] = train_df['predicted_meter_waiting_diff'] / (train_df['predicted_meter_waiting'] + 1)\n",
    "test_df['predicted_meter_waiting_diff_per_predicted_meter_waiting'] = test_df['predicted_meter_waiting_diff'] / (test_df['predicted_meter_waiting'] + 1)\n",
    "\n",
    "train_df['meter_waiting_per_duration'] = train_df['meter_waiting'] / (train_df['duration']+1)\n",
    "test_df['meter_waiting_per_duration'] = test_df['meter_waiting'] / (test_df['duration']+1)\n",
    "\n",
    "train_df['predicted_meter_waiting_per_duration'] = train_df['predicted_meter_waiting'] / (train_df['predicted_duration']+1)\n",
    "test_df['predicted_meter_waiting_per_duration'] = test_df['predicted_meter_waiting'] / (test_df['predicted_duration']+1)\n",
    "\n",
    "train_df['predicted_meter_waiting_per_duration_diff'] = train_df['meter_waiting_per_duration'] - train_df['predicted_meter_waiting_per_duration']\n",
    "test_df['predicted_meter_waiting_per_duration_diff'] = test_df['meter_waiting_per_duration'] - test_df['predicted_meter_waiting_per_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['duration','meter_waiting','fare','is_more_than_one_day','cal_time_difference']\n",
    "\n",
    "X = data[cols].values\n",
    "y = data['meter_waiting_fare'].values\n",
    "\n",
    "X_train_df = train_df[cols].values\n",
    "X_test_df = test_df[cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_df = scaler.transform(X_train_df)\n",
    "X_test_df = scaler.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "\n",
    "validation_scores = []\n",
    "models = []\n",
    "\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "kf = KFold(n_splits=folds)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test,pred) ** 0.5\n",
    "    validation_scores.append(score)\n",
    "    models.append(model)\n",
    "    print('RMSE:', score)\n",
    "    \n",
    "    train_preds += model.predict(X_train_df)\n",
    "    test_preds += model.predict(X_test_df)\n",
    "    \n",
    "train_preds /= folds\n",
    "test_preds /= folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_meter_waiting_fare'] = train_preds\n",
    "test_df['predicted_meter_waiting_fare'] = test_preds\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_diff'] = train_df['meter_waiting_fare'] - train_df['predicted_meter_waiting_fare']\n",
    "test_df['predicted_meter_waiting_fare_diff'] = test_df['meter_waiting_fare'] - test_df['predicted_meter_waiting_fare']\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_diff_per_meter_waiting_fare'] = train_df['predicted_meter_waiting_fare_diff'] / (train_df['meter_waiting_fare']+1)\n",
    "test_df['predicted_meter_waiting_fare_diff_per_meter_waiting_fare'] = test_df['predicted_meter_waiting_fare_diff'] / (test_df['meter_waiting_fare']+1)\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_diff_per_distance'] = train_df['predicted_meter_waiting_fare_diff'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_meter_waiting_fare_diff_per_distance'] = test_df['predicted_meter_waiting_fare_diff'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare'] = train_df['predicted_meter_waiting_fare_diff'] / (train_df['predicted_meter_waiting_fare']+1)\n",
    "test_df['predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare'] = test_df['predicted_meter_waiting_fare_diff'] / (test_df['predicted_meter_waiting_fare']+1)\n",
    "\n",
    "train_df['meter_waiting_fare_per_meter_waiting'] = train_df['meter_waiting_fare'] / train_df['meter_waiting']\n",
    "test_df['meter_waiting_fare_per_meter_waiting'] = test_df['meter_waiting_fare'] / test_df['meter_waiting']\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_per_meter_waiting'] = train_df['predicted_meter_waiting_fare'] / train_df['predicted_meter_waiting']\n",
    "test_df['predicted_meter_waiting_fare_per_meter_waiting'] = test_df['predicted_meter_waiting_fare'] / test_df['predicted_meter_waiting']\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_per_meter_waiting_diff'] = train_df['meter_waiting_fare_per_meter_waiting'] - train_df['predicted_meter_waiting_fare_per_meter_waiting']\n",
    "test_df['predicted_meter_waiting_fare_per_meter_waiting_diff'] = test_df['meter_waiting_fare_per_meter_waiting'] - test_df['predicted_meter_waiting_fare_per_meter_waiting']\n",
    "\n",
    "train_df['meter_waiting_fare_per_duration'] = train_df['meter_waiting_fare'] / train_df['duration']\n",
    "test_df['meter_waiting_fare_per_duration'] = test_df['meter_waiting_fare'] / test_df['duration']\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_per_duration'] = train_df['predicted_meter_waiting_fare'] / train_df['predicted_duration']\n",
    "test_df['predicted_meter_waiting_fare_per_duration'] = test_df['predicted_meter_waiting_fare'] / test_df['predicted_duration']\n",
    "\n",
    "train_df['predicted_meter_waiting_fare_per_duration_diff'] = train_df['meter_waiting_fare_per_duration'] - train_df['predicted_meter_waiting_fare_per_duration']\n",
    "test_df['predicted_meter_waiting_fare_per_duration_diff'] = test_df['meter_waiting_fare_per_duration'] - test_df['predicted_meter_waiting_fare_per_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['meter_waiting_fare_per_duration','meter_waiting_per_duration','fare_per_duration']\n",
    "data = train_df[train_df['label'] == 1].dropna()\n",
    "\n",
    "X = data[cols].values\n",
    "y = data['additional_fare'].values\n",
    "\n",
    "X_train_df = train_df[cols].values\n",
    "X_test_df = test_df[cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_df = np.nan_to_num(scaler.transform(X_train_df))\n",
    "X_test_df = np.nan_to_num(scaler.transform(X_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "\n",
    "validation_scores = []\n",
    "models = []\n",
    "\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "kf = KFold(n_splits=folds)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#     model = CatBoostRegressor(**params)\n",
    "#     model.fit(X=X_train,y=y_train,eval_set=(X_test,y_test))\n",
    "   \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test,pred) ** 0.5\n",
    "    validation_scores.append(score)\n",
    "    models.append(model)\n",
    "    print('RMSE:', score)\n",
    "    \n",
    "    train_preds += model.predict(X_train_df)\n",
    "    test_preds += model.predict(X_test_df)\n",
    "    \n",
    "train_preds /= folds\n",
    "test_preds /= folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_additional_fare'] = train_preds\n",
    "test_df['predicted_additional_fare'] = test_preds\n",
    "\n",
    "train_df['predicted_additional_fare_diff'] = train_df['additional_fare'] - train_df['predicted_additional_fare']\n",
    "test_df['predicted_additional_fare_diff'] = test_df['additional_fare'] - test_df['predicted_additional_fare']\n",
    "\n",
    "train_df['predicted_additional_fare_diff_per_additional_fare'] = train_df['predicted_additional_fare_diff'] / (train_df['additional_fare']+1)\n",
    "test_df['predicted_additional_fare_diff_per_additional_fare'] = test_df['predicted_additional_fare_diff'] / (test_df['additional_fare']+1)\n",
    "\n",
    "train_df['predicted_addtional_fare_per_fare'] = train_df['predicted_additional_fare'] / (train_df['predicted_fare']+1)\n",
    "test_df['predicted_addtional_fare_per_fare'] = test_df['predicted_additional_fare'] / (test_df['predicted_fare']+1)\n",
    "\n",
    "train_df['addtional_fare_per_fare'] = train_df['additional_fare'] / (train_df['fare']+1)\n",
    "test_df['addtional_fare_per_fare'] = test_df['additional_fare'] / (test_df['fare']+1)\n",
    "\n",
    "train_df['addtional_fare_per_distance'] = train_df['additional_fare'] / (train_df['distance_km']+1)\n",
    "test_df['addtional_fare_per_distance'] = test_df['additional_fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_addtional_fare_per_distance'] = train_df['predicted_additional_fare'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_addtional_fare_per_distance'] = test_df['predicted_additional_fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_addtional_fare_diff_per_distance'] = train_df['predicted_additional_fare_diff'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_addtional_fare_diff_per_distance'] = test_df['predicted_additional_fare_diff'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['addtional_fare_per_duration'] = train_df['additional_fare'] / (train_df['duration']+1)\n",
    "test_df['addtional_fare_per_duration'] = test_df['additional_fare'] / (test_df['duration']+1)\n",
    "\n",
    "train_df['predicted_addtional_fare_per_duration'] = train_df['predicted_additional_fare'] / (train_df['predicted_duration']+1)\n",
    "test_df['predicted_addtional_fare_per_duration'] = test_df['predicted_additional_fare'] / (test_df['predicted_duration']+1)\n",
    "\n",
    "train_df['fare-additional_fare'] = train_df['fare'] - train_df['additional_fare']\n",
    "test_df['fare-additional_fare'] = test_df['fare'] - test_df['additional_fare']\n",
    "\n",
    "train_df['predicted_fare-additional_fare'] = train_df['predicted_fare'] - train_df['predicted_additional_fare']\n",
    "test_df['predicted_fare-additional_fare'] = test_df['predicted_fare'] - test_df['predicted_additional_fare']\n",
    "\n",
    "train_df['fare-additional_fare-meter_waiting_fare'] = train_df['fare'] - (train_df['additional_fare'] + train_df['meter_waiting_fare'])\n",
    "test_df['fare-additional_fare-meter_waiting_fare'] = test_df['fare'] - (test_df['additional_fare'] + test_df['meter_waiting_fare'])\n",
    "\n",
    "train_df['predicted_fare-additional_fare-meter_waiting_fare'] = train_df['predicted_fare'] - (train_df['predicted_additional_fare'] + train_df['predicted_meter_waiting_fare'])\n",
    "test_df['predicted_fare-additional_fare-meter_waiting_fare'] = test_df['predicted_fare'] - (test_df['predicted_additional_fare'] + test_df['predicted_meter_waiting_fare'])\n",
    "\n",
    "train_df['fare-additional_fare_per_distance'] = train_df['fare-additional_fare'] / (train_df['distance_km']+1)\n",
    "test_df['fare-additional_fare_per_distance'] = test_df['fare-additional_fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_fare-additional_fare_per_distance'] = train_df['predicted_fare-additional_fare'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_fare-additional_fare_per_distance'] = test_df['predicted_fare-additional_fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['fare-additional_fare_per_duration'] = train_df['fare-additional_fare'] / (train_df['duration']+1)\n",
    "test_df['fare-additional_fare_per_duration'] = test_df['fare-additional_fare'] / (test_df['duration']+1)\n",
    "\n",
    "train_df['predicted_fare-additional_fare_per_duration'] = train_df['predicted_fare-additional_fare'] / (train_df['predicted_duration']+1)\n",
    "test_df['predicted_fare-additional_fare_per_duration'] = test_df['predicted_fare-additional_fare'] / (test_df['predicted_duration']+1)\n",
    "\n",
    "train_df['fare-additional_fare-meter_waiting_fare_per_distance'] = train_df['fare-additional_fare-meter_waiting_fare'] / (train_df['distance_km']+1)\n",
    "test_df['fare-additional_fare-meter_waiting_fare_per_distance'] = test_df['fare-additional_fare-meter_waiting_fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['predicted_fare-additional_fare-meter_waiting_fare_per_distance'] = train_df['predicted_fare-additional_fare-meter_waiting_fare'] / (train_df['distance_km']+1)\n",
    "test_df['predicted_fare-additional_fare-meter_waiting_fare_per_distance'] = test_df['predicted_fare-additional_fare-meter_waiting_fare'] / (test_df['distance_km']+1)\n",
    "\n",
    "train_df['fare-additional_fare-meter_waiting_fare_per_duration'] = train_df['fare-additional_fare-meter_waiting_fare'] / (train_df['duration']+1)\n",
    "test_df['fare-additional_fare-meter_waiting_fare_per_duration'] = test_df['fare-additional_fare-meter_waiting_fare'] / (test_df['duration']+1)\n",
    "\n",
    "train_df['predicted_fare-additional_fare-meter_waiting_fare_per_duration'] = train_df['predicted_fare-additional_fare-meter_waiting_fare'] / (train_df['predicted_duration']+1)\n",
    "test_df['predicted_fare-additional_fare-meter_waiting_fare_per_duration'] = test_df['predicted_fare-additional_fare-meter_waiting_fare'] / (test_df['predicted_duration']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df[train_df['label'] == 1].dropna()\n",
    "y = data['meter_waiting_till_pickup'].values\n",
    "X = data.drop(['label','meter_waiting_till_pickup'],axis=1)\n",
    "cols = X.columns\n",
    "X = X.values\n",
    "\n",
    "X_train_df = train_df[cols].values\n",
    "X_test_df = test_df[cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_df = np.nan_to_num(scaler.transform(X_train_df))\n",
    "X_test_df = np.nan_to_num(scaler.transform(X_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss_function':'RMSE',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'RMSE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "validation_scores = []\n",
    "models = []\n",
    "\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "kf = KFold(n_splits=folds)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X=X_train,y=y_train,eval_set=(X_test,y_test),verbose=10)\n",
    "   \n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test,pred) ** 0.5\n",
    "    validation_scores.append(score)\n",
    "    models.append(model)\n",
    "    print('RMSE:', score)\n",
    "    \n",
    "    train_preds += model.predict(X_train_df)\n",
    "    test_preds += model.predict(X_test_df)\n",
    "    \n",
    "train_preds /= folds\n",
    "test_preds /= folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_meter_waiting_till_pickup'] = train_preds\n",
    "test_df['predicted_meter_waiting_till_pickup'] = test_preds\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_diff'] = train_df['meter_waiting_till_pickup'] - train_df['predicted_meter_waiting_till_pickup']\n",
    "test_df['predicted_meter_waiting_till_pickup_diff'] = test_df['meter_waiting_till_pickup'] - test_df['predicted_meter_waiting_till_pickup']\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_diff_per_meter_waiting_till_pickup'] = train_df['predicted_meter_waiting_till_pickup_diff'] / (train_df['meter_waiting_till_pickup']+1)\n",
    "test_df['predicted_meter_waiting_till_pickup_diff_per_meter_waiting_till_pickup'] = test_df['predicted_meter_waiting_till_pickup_diff'] / (test_df['meter_waiting_till_pickup']+1)\n",
    "\n",
    "train_df['meter_waiting_till_pickup_per_meter_waiting'] = train_df['meter_waiting_till_pickup'] / (train_df['meter_waiting'] + 1)\n",
    "test_df['meter_waiting_till_pickup_per_meter_waiting'] = test_df['meter_waiting_till_pickup'] / (test_df['meter_waiting'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_per_meter_waiting'] = train_df['predicted_meter_waiting_till_pickup'] / (train_df['predicted_meter_waiting'] + 1)\n",
    "test_df['predicted_meter_waiting_till_pickup_per_meter_waiting'] = test_df['predicted_meter_waiting_till_pickup'] / (test_df['predicted_meter_waiting'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_per_meter_waiting_diff'] = train_df['meter_waiting_till_pickup_per_meter_waiting'] - train_df['predicted_meter_waiting_till_pickup_per_meter_waiting']\n",
    "test_df['predicted_meter_waiting_till_pickup_per_meter_waiting_diff'] = test_df['meter_waiting_till_pickup_per_meter_waiting'] - test_df['predicted_meter_waiting_till_pickup_per_meter_waiting']\n",
    "\n",
    "train_df['meter_waiting_after_pickup'] = train_df['meter_waiting'] - train_df['meter_waiting_till_pickup']\n",
    "test_df['meter_waiting_after_pickup'] = test_df['meter_waiting'] - test_df['meter_waiting_till_pickup']\n",
    "\n",
    "train_df['predicted_meter_waiting_after_pickup'] = train_df['predicted_meter_waiting'] - train_df['predicted_meter_waiting_till_pickup']\n",
    "test_df['predicted_meter_waiting_after_pickup'] = test_df['predicted_meter_waiting'] - test_df['predicted_meter_waiting_till_pickup']\n",
    "\n",
    "train_df['meter_waiting_after_pickup_per_duration'] = train_df['meter_waiting_after_pickup'] / (train_df['duration'] + 1)\n",
    "test_df['meter_waiting_after_pickup_per_duration'] = test_df['meter_waiting_after_pickup'] / (test_df['duration'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_after_pickup_per_duration'] = train_df['predicted_meter_waiting_after_pickup'] / (train_df['predicted_duration'] + 1)\n",
    "test_df['predicted_meter_waiting_after_pickup_per_duration'] = test_df['predicted_meter_waiting_after_pickup'] / (test_df['predicted_duration'] + 1)\n",
    "\n",
    "train_df['meter_waiting_till_pickup_per_duration'] = train_df['meter_waiting_till_pickup'] / (train_df['duration'] + 1)\n",
    "test_df['meter_waiting_till_pickup_per_duration'] = test_df['meter_waiting_till_pickup'] / (test_df['duration'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_per_duration'] = train_df['predicted_meter_waiting_till_pickup'] / (train_df['predicted_duration'] + 1)\n",
    "test_df['predicted_meter_waiting_till_pickup_per_duration'] = test_df['predicted_meter_waiting_till_pickup'] / (test_df['predicted_duration'] + 1)\n",
    "\n",
    "train_df['meter_waiting_till_pickup_per_distance'] = train_df['meter_waiting_till_pickup'] / (train_df['distance_km'] + 1)\n",
    "test_df['meter_waiting_till_pickup_per_distance'] = test_df['meter_waiting_till_pickup'] / (test_df['distance_km'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_per_distance'] = train_df['predicted_meter_waiting_till_pickup'] / (train_df['distance_km'] + 1)\n",
    "test_df['predicted_meter_waiting_till_pickup_per_distance'] = test_df['predicted_meter_waiting_till_pickup'] / (test_df['distance_km'] + 1)\n",
    "\n",
    "train_df['meter_waiting_after_pickup_per_distance'] = train_df['meter_waiting_after_pickup'] / (train_df['distance_km'] + 1)\n",
    "test_df['meter_waiting_after_pickup_per_distance'] = test_df['meter_waiting_after_pickup'] / (test_df['distance_km'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_after_pickup_per_distance'] = train_df['predicted_meter_waiting_after_pickup'] / (train_df['distance_km'] + 1)\n",
    "test_df['predicted_meter_waiting_after_pickup_per_distance'] = test_df['predicted_meter_waiting_after_pickup'] / (test_df['distance_km'] + 1)\n",
    "\n",
    "train_df['meter_waiting_till_pickup_per_fare'] = train_df['meter_waiting_till_pickup'] / (train_df['fare'] + 1)\n",
    "test_df['meter_waiting_till_pickup_per_fare'] = test_df['meter_waiting_till_pickup'] / (test_df['fare'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_per_fare'] = train_df['predicted_meter_waiting_till_pickup'] / (train_df['predicted_fare'] + 1)\n",
    "test_df['predicted_meter_waiting_till_pickup_per_fare'] = test_df['predicted_meter_waiting_till_pickup'] / (test_df['predicted_fare'] + 1)\n",
    "\n",
    "train_df['meter_waiting_after_pickup_per_fare'] = train_df['meter_waiting_after_pickup'] / (train_df['fare'] + 1)\n",
    "test_df['meter_waiting_after_pickup_per_fare'] = test_df['meter_waiting_after_pickup'] / (test_df['fare'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_after_pickup_per_fare'] = train_df['predicted_meter_waiting_after_pickup'] / (train_df['predicted_fare'] + 1)\n",
    "test_df['predicted_meter_waiting_after_pickup_per_fare'] = test_df['predicted_meter_waiting_after_pickup'] / (test_df['predicted_fare'] + 1)\n",
    "\n",
    "train_df['meter_waiting_till_pickup_per_meter_waiting_fare'] = train_df['meter_waiting_till_pickup'] / (train_df['meter_waiting_fare'] + 1)\n",
    "test_df['meter_waiting_till_pickup_per_meter_waiting_fare'] = test_df['meter_waiting_till_pickup'] / (test_df['meter_waiting_fare'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_till_pickup_per_meter_waiting_fare'] = train_df['predicted_meter_waiting_till_pickup'] / (train_df['predicted_meter_waiting_fare'] + 1)\n",
    "test_df['predicted_meter_waiting_till_pickup_per_meter_waiting_fare'] = test_df['predicted_meter_waiting_till_pickup'] / (test_df['predicted_meter_waiting_fare'] + 1)\n",
    "\n",
    "train_df['meter_waiting_after_pickup_per_meter_waiting_fare'] = train_df['meter_waiting_after_pickup'] / (train_df['meter_waiting_fare'] + 1)\n",
    "test_df['meter_waiting_after_pickup_per_meter_waiting_fare'] = test_df['meter_waiting_after_pickup'] / (test_df['meter_waiting_fare'] + 1)\n",
    "\n",
    "train_df['predicted_meter_waiting_after_pickup_per_meter_waiting_fare'] = train_df['predicted_meter_waiting_after_pickup'] / (train_df['predicted_meter_waiting_fare'] + 1)\n",
    "test_df['predicted_meter_waiting_after_pickup_per_meter_waiting_fare'] = test_df['predicted_meter_waiting_after_pickup'] / (test_df['predicted_meter_waiting_fare'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anomaly = pd.read_csv('train_df_anomaly.csv')\n",
    "test_anomaly = pd.read_csv('test_df_anomaly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_columns = [\n",
    "    'fare_anomaly',\n",
    "    'additional_fare_anomaly', \n",
    "    'duration_anomaly',\n",
    "    'meter_waiting_anomaly', \n",
    "    'meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_till_pickup_anomaly', \n",
    "    'additional_fare_duration_anomaly',\n",
    "    'additional_fare_meter_waiting_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_anomaly', \n",
    "    'duration_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in anomaly_columns:\n",
    "    train_df[col] = 1-train_anomaly[col]\n",
    "    test_df[col] = 1-test_anomaly[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_multiplicatives = {\n",
    "    'fare_anomaly':[\n",
    "        'predicted_fare_diff',\n",
    "        'predicted_fare_diff_per_fare',\n",
    "        'predicted_fare_diff_per_distance',\n",
    "    ],\n",
    "    'additional_fare_anomaly':[\n",
    "        'predicted_additional_fare_diff',\n",
    "        'predicted_additional_fare_diff_per_additional_fare',\n",
    "        'predicted_addtional_fare_per_distance',\n",
    "    ],\n",
    "    'duration_anomaly':[\n",
    "        'predicted_duration_diff', \n",
    "        'predicted_duraton_diff_per_duraton',\n",
    "        'predicted_duraton_diff_per_distance', \n",
    "    ],\n",
    "    'meter_waiting_anomaly':[\n",
    "        'predicted_meter_waiting_diff',\n",
    "        'predicted_meter_waiting_diff_per_meter_waiting',\n",
    "        'predicted_meter_waiting_diff_per_distance'\n",
    "    ],\n",
    "    'meter_waiting_fare_anomaly':[\n",
    "        'predicted_meter_waiting_fare_diff',\n",
    "        'predicted_meter_waiting_fare_diff_per_meter_waiting_fare',\n",
    "        'predicted_meter_waiting_fare_diff_per_distance'\n",
    "    ]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "for col1 in anomaly_multiplicatives:\n",
    "    for col2 in anomaly_multiplicatives[col1]:\n",
    "        name = f'{col1}_{col2}_prod'\n",
    "        train_df[name] = train_df[col1] * train_df[col2]\n",
    "        test_df[name] = test_df[col1] * test_df[col2]\n",
    "        new_cols.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['predicted_duraton_diff_per_duraton@predicted_duraton_diff_per_distance'] = train_df['predicted_duraton_diff_per_duraton'] * train_df['predicted_duraton_diff_per_distance']\n",
    "test_df['predicted_duraton_diff_per_duraton@predicted_duraton_diff_per_distance'] = test_df['predicted_duraton_diff_per_duraton'] * test_df['predicted_duraton_diff_per_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_diff(col_name):\n",
    "    normalizer = StandardScaler()\n",
    "    normalizer.fit(train_df[train_df['label'] == 1][col_name].values.reshape(-1,1))\n",
    "\n",
    "    train_df[f'{col_name}_normalized'] = normalizer.transform(train_df[col_name].values.reshape(-1,1))\n",
    "    test_df[f'{col_name}_normalized'] = normalizer.transform(test_df[col_name].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols = [\n",
    "    'predicted_fare_diff',\n",
    "    'predicted_fare_diff_per_fare',\n",
    "    'predicted_fare_diff_per_predicted_fare', \n",
    "    'predicted_fare_diff_per_distance',\n",
    "    'predicted_duraton_diff_per_duraton',\n",
    "    'predicted_duraton_diff_per_predicted_duration', \n",
    "    'predicted_fare_per_duration_diff',\n",
    "    'predicted_avg_speed_diff',\n",
    "    'predicted_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting',\n",
    "    'predicted_meter_waiting_per_duration_diff',\n",
    "    'predicted_meter_waiting_fare_diff',\n",
    "    'predicted_meter_waiting_fare_diff_per_meter_waiting_fare',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff',\n",
    "    'predicted_additional_fare_diff',\n",
    "    'predicted_additional_fare_diff_per_additional_fare'\n",
    "]\n",
    "for col in diff_cols:\n",
    "    normalize_diff(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_value(col):\n",
    "    grouping_order = ['pick_cluster','pickup_timeslot']\n",
    "    group = train_df[train_df['label'] == 1].groupby(grouping_order)[col].mean()\n",
    "    def f(row):\n",
    "        return group[row['pick_cluster']][row['pickup_timeslot']]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoding(col):\n",
    "    train_df[f'{col}_mean'] = train_df.apply(mean_value(col),axis=1)\n",
    "    test_df[f'{col}_mean'] = test_df.apply(mean_value(col),axis=1)\n",
    "    \n",
    "    train_df[f'{col}_mean_diff'] = train_df[f'{col}_mean'] - train_df[col]\n",
    "    test_df[f'{col}_mean_diff'] = test_df[f'{col}_mean'] - test_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols = [\n",
    "    'fare_per_distance',\n",
    "    'avg_speed', \n",
    "    'meter_waiting_per_duration', \n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_fare_per_duration',\n",
    "    'addtional_fare_per_fare',\n",
    "    'addtional_fare_per_distance', \n",
    "    'addtional_fare_per_duration'\n",
    "]\n",
    "for col in mean_cols:\n",
    "    mean_encoding(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'predicted_fare_diff',\n",
    "    'predicted_duration_diff',\n",
    "    'predicted_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_fare_diff',\n",
    "    'predicted_additional_fare_diff',    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_bucket(column):\n",
    "    std = train_df[train_df['label']==1][column].std()\n",
    "    name = f'{column}_bucket'\n",
    "    train_df[name] = np.round((train_df[column]/std)+1).astype(int)\n",
    "    test_df[name] = np.round((test_df[column]/std)+1).astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in cols:\n",
    "    col_bucket(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_multiplicatives = {\n",
    "    'predicted_fare_diff_bucket':[\n",
    "        'fare',\n",
    "        'predicted_fare',\n",
    "        'fare_per_distance',\n",
    "        'predicted_fare_per_distance',         \n",
    "    ],\n",
    "    'predicted_duration_diff_bucket':[\n",
    "        'duration',\n",
    "        'predicted_duration',\n",
    "        'avg_speed', \n",
    "        'predicted_avg_speed',         \n",
    "    ],\n",
    "    'predicted_meter_waiting_diff_bucket':[\n",
    "        'meter_waiting', \n",
    "        'predicted_meter_waiting', \n",
    "        'meter_waiting_per_duration', \n",
    "        'predicted_meter_waiting_per_duration',\n",
    "    ],\n",
    "    'predicted_meter_waiting_fare_diff_bucket':[\n",
    "        'meter_waiting_fare',\n",
    "        'predicted_meter_waiting_fare',\n",
    "        'meter_waiting_fare_per_meter_waiting',\n",
    "        'predicted_meter_waiting_fare_per_meter_waiting',\n",
    "        'meter_waiting_fare_per_duration',\n",
    "        'predicted_meter_waiting_fare_per_duration',\n",
    "    ],\n",
    "    'predicted_additional_fare_diff':[\n",
    "        'additional_fare',\n",
    "        'predicted_additional_fare', \n",
    "        'predicted_addtional_fare_per_fare', \n",
    "        'addtional_fare_per_fare',\n",
    "        'addtional_fare_per_distance', \n",
    "        'predicted_addtional_fare_per_distance',\n",
    "        'addtional_fare_per_duration', \n",
    "        'predicted_addtional_fare_per_duration',\n",
    "    ]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in tqdm(bin_multiplicatives):\n",
    "    for col in bin_multiplicatives[bucket]:\n",
    "        name = f'{bucket}@{col}'\n",
    "        train_df[name] = train_df[bucket] * train_df[col]\n",
    "        test_df[name] = test_df[bucket] * test_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df_final.csv',index=False)\n",
    "test_df.to_csv('test_df_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df_final.csv')\n",
    "train_df = train_df.fillna(0)\n",
    "test_df = pd.read_csv('test_df_final.csv')\n",
    "test_df = test_df.fillna(0)\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cat_cols = [\n",
    "    'pickup_hour',\n",
    "    'drop_hour',\n",
    "#     'pick_cluster'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'fare_per_distance',\n",
    "    'fare_per_duration',\n",
    "    'avg_speed',\n",
    "    'meter_waiting_per_duration',\n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_fare_per_duration',\n",
    "    'addtional_fare_per_fare',\n",
    "    'addtional_fare_per_distance',\n",
    "    'addtional_fare_per_duration',\n",
    "    'fare-additional_fare_per_distance',\n",
    "    'fare-additional_fare_per_duration',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_distance',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_duration',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting',\n",
    "    'meter_waiting_after_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_distance',\n",
    "    'meter_waiting_after_pickup_per_distance',\n",
    "    'meter_waiting_till_pickup_per_fare',\n",
    "    'meter_waiting_after_pickup_per_fare',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting_fare',\n",
    "    'meter_waiting_after_pickup_per_meter_waiting_fare',    \n",
    "    'meter_waiting_till_pickup'\n",
    "]\n",
    "\n",
    "cat_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += original_cat_cols\n",
    "cat_features += original_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumeric_cols_to_multiply = [\n",
    "    'fare_per_distance',    \n",
    "    'avg_speed',    \n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_till_pickup'\n",
    "]\n",
    "\n",
    "encoding_cols = []\n",
    "for col1 in original_cat_cols:\n",
    "    for col2 in neumeric_cols_to_multiply:\n",
    "        name = f'{col1}@{col2}'\n",
    "        train_df[name] = train_df[col1] * train_df[col2]\n",
    "        test_df[name] = test_df[col1] * test_df[col2]\n",
    "        encoding_cols.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += encoding_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "models = []\n",
    "train_pools = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    train_pools.append(train_pool)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class += model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    models.append(model)\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['prediction'] = test_class\n",
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df_final.csv')\n",
    "train_df = train_df.fillna(0)\n",
    "test_df = pd.read_csv('test_df_final.csv')\n",
    "test_df = test_df.fillna(0)\n",
    "submission_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "y = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cat_cols = [\n",
    "    'pickup_hour',\n",
    "    'drop_hour',\n",
    "#     'pick_cluster'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'fare_per_distance',\n",
    "    'fare_per_duration',\n",
    "    'avg_speed',\n",
    "    'meter_waiting_per_duration',\n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_fare_per_duration',\n",
    "    'addtional_fare_per_fare',\n",
    "    'addtional_fare_per_distance',\n",
    "    'addtional_fare_per_duration',\n",
    "    'fare-additional_fare_per_distance',\n",
    "    'fare-additional_fare_per_duration',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_distance',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_duration',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting',\n",
    "    'meter_waiting_after_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_distance',\n",
    "    'meter_waiting_after_pickup_per_distance',\n",
    "    'meter_waiting_till_pickup_per_fare',\n",
    "    'meter_waiting_after_pickup_per_fare',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting_fare',\n",
    "    'meter_waiting_after_pickup_per_meter_waiting_fare',    \n",
    "    'meter_waiting_till_pickup',\n",
    "    \n",
    "    'additional_fare',\n",
    "    'duration',\n",
    "    'meter_waiting',\n",
    "    'meter_waiting_fare',\n",
    "    \n",
    "    'predicted_duration_diff',\n",
    "    'predicted_fare_diff_per_predicted_fare',\n",
    "    'predicted_fare_diff_per_fare',\n",
    "    'predicted_fare_diff_per_distance',\n",
    "]\n",
    "\n",
    "cat_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += original_cat_cols\n",
    "cat_features += original_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumeric_cols_to_multiply = [\n",
    "    'fare_per_distance',  \n",
    "#     'fare_per_duration',\n",
    "#     'fare-additional_fare_per_duration',\n",
    "    'avg_speed',    \n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_till_pickup'\n",
    "]\n",
    "\n",
    "encoding_cols = []\n",
    "for col1 in original_cat_cols:\n",
    "    for col2 in neumeric_cols_to_multiply:\n",
    "        name = f'{col1}@{col2}'\n",
    "        train_df[name] = train_df[col1] * train_df[col2]\n",
    "        test_df[name] = test_df[col1] * test_df[col2]\n",
    "        encoding_cols.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_features = []\n",
    "\n",
    "train_df['pickup_timeslot@distance'] = (train_df['pickup_timeslot']+1) * train_df['distance_km']\n",
    "test_df['pickup_timeslot@distance'] = (test_df['pickup_timeslot']+1) * test_df['distance_km']\n",
    "special_features.append('pickup_timeslot@distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += encoding_cols\n",
    "features += special_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "models = []\n",
    "train_pools = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    train_pools.append(train_pool)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class += model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    models.append(model)\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['prediction'] = test_class\n",
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat,average='micro'), True\n",
    "\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = 1-f1_score(y_true, np.round(y_pred),average='micro')\n",
    "    return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df_final.csv')\n",
    "train_df = train_df.fillna(0)\n",
    "test_df = pd.read_csv('test_df_final.csv')\n",
    "test_df = test_df.fillna(0)\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions_train = pd.DataFrame()\n",
    "model_predictions_train['label'] = train_df['label']\n",
    "model_predictions_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'fare_anomaly',\n",
    "    'additional_fare_anomaly', \n",
    "    'duration_anomaly',\n",
    "    'meter_waiting_anomaly', \n",
    "    'meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_till_pickup_anomaly', \n",
    "    'additional_fare_duration_anomaly',\n",
    "    'additional_fare_meter_waiting_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_anomaly', \n",
    "    'duration_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    'fare_anomaly',\n",
    "    'additional_fare_anomaly', \n",
    "    'duration_anomaly',\n",
    "    'meter_waiting_anomaly', \n",
    "    'meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_till_pickup_anomaly', \n",
    "    'additional_fare_duration_anomaly',\n",
    "    'additional_fare_meter_waiting_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_anomaly', \n",
    "    'duration_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_fare_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_duration_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'additional_fare_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "    'duration_meter_waiting_meter_waiting_fare_meter_waiting_till_pickup_anomaly',\n",
    "]\n",
    "\n",
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "}\n",
    "\n",
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)\n",
    "\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class += model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    models.append(model)\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':15,\n",
    "    'weights':'uniform'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators':50,\n",
    "    'max_depth':10,\n",
    "    'random_state':0,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'rf_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':2,\n",
    "    'kernel':'linear',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svc_linear_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':2,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svc_rbf_anomaly'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'additional_fare',\n",
    "    'duration',\n",
    "    'meter_waiting',\n",
    "    'meter_waiting_fare',\n",
    "    'meter_waiting_till_pickup',\n",
    "    'fare',\n",
    "    'pickup_date',\n",
    "    'pickup_hour',\n",
    "    'pickup_minute',\n",
    "    'drop_date',\n",
    "    'drop_hour',\n",
    "    'drop_minute',\n",
    "    'pick_cluster',\n",
    "    'is_more_than_one_day',\n",
    "    'distance_km',\n",
    "    'fare_per_km',\n",
    "    'pickup_timeslot',\n",
    "    'day_of_week',\n",
    "    'is_weekday',\n",
    "    'cal_time_difference',\n",
    "    'fare_per_distance',\n",
    "    'fare_per_duration',\n",
    "    'avg_speed',\n",
    "    'meter_waiting_per_duration',\n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_fare_per_duration',\n",
    "    'addtional_fare_per_fare',\n",
    "    'addtional_fare_per_distance',\n",
    "    'addtional_fare_per_duration',\n",
    "    'fare-additional_fare',\n",
    "    'fare-additional_fare-meter_waiting_fare',\n",
    "    'fare-additional_fare_per_distance',\n",
    "    'fare-additional_fare_per_duration',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_distance',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_duration',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting',\n",
    "    'meter_waiting_after_pickup',\n",
    "    'meter_waiting_after_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_distance',\n",
    "    'meter_waiting_after_pickup_per_distance',\n",
    "    'meter_waiting_till_pickup_per_fare',\n",
    "    'meter_waiting_after_pickup_per_fare',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting_fare',\n",
    "    'meter_waiting_after_pickup_per_meter_waiting_fare',\n",
    "    'fare_per_distance_mean',\n",
    "    'fare_per_distance_mean_diff',\n",
    "    'avg_speed_mean',\n",
    "    'avg_speed_mean_diff',\n",
    "    'meter_waiting_per_duration_mean',\n",
    "    'meter_waiting_per_duration_mean_diff',\n",
    "    'meter_waiting_fare_per_meter_waiting_mean',\n",
    "    'meter_waiting_fare_per_meter_waiting_mean_diff',\n",
    "    'meter_waiting_fare_per_duration_mean',\n",
    "    'meter_waiting_fare_per_duration_mean_diff',\n",
    "    'addtional_fare_per_fare_mean',\n",
    "    'addtional_fare_per_fare_mean_diff',\n",
    "    'addtional_fare_per_distance_mean',\n",
    "    'addtional_fare_per_distance_mean_diff',\n",
    "    'addtional_fare_per_duration_mean',\n",
    "    'addtional_fare_per_duration_mean_diff',\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    'pickup_date',\n",
    "    'pickup_hour',\n",
    "    'pickup_minute',\n",
    "    'drop_date',\n",
    "    'drop_hour',\n",
    "    'drop_minute',\n",
    "    'pick_cluster',\n",
    "    'is_more_than_one_day',\n",
    "    'pickup_timeslot',\n",
    "    'day_of_week',\n",
    "    'is_weekday',\n",
    "]\n",
    "\n",
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class += model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    models.append(model)\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_base'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_base'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'predicted_fare_diff',\n",
    "    'predicted_fare_diff_per_fare',\n",
    "    'predicted_fare_diff_per_predicted_fare',\n",
    "    'predicted_fare_diff_per_distance',\n",
    "    'predicted_duration_diff',\n",
    "    'predicted_duraton_diff_per_duraton',\n",
    "    'predicted_duraton_diff_per_predicted_duration',\n",
    "    'predicted_duraton_diff_per_distance',\n",
    "    'predicted_fare_per_duration_diff',\n",
    "    'predicted_avg_speed_diff',\n",
    "    'predicted_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting',\n",
    "    'predicted_meter_waiting_diff_per_distance',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting',\n",
    "    'predicted_meter_waiting_per_duration_diff',\n",
    "    'predicted_meter_waiting_fare_diff',\n",
    "    'predicted_meter_waiting_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff',\n",
    "    'predicted_additional_fare_diff',\n",
    "    'predicted_additional_fare_diff_per_additional_fare',\n",
    "    'predicted_addtional_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_till_pickup_diff',\n",
    "    'predicted_meter_waiting_till_pickup_diff_per_meter_waiting_till_pickup',\n",
    "    'predicted_meter_waiting_till_pickup_per_meter_waiting_diff',\n",
    "    'predicted_fare_diff_per_distance_normalized',\n",
    "    'predicted_fare_diff_normalized',\n",
    "    'predicted_fare_diff_per_fare_normalized',\n",
    "    'predicted_fare_diff_per_predicted_fare_normalized',\n",
    "    'predicted_duraton_diff_per_duraton_normalized',\n",
    "    'predicted_duraton_diff_per_predicted_duration_normalized',\n",
    "    'predicted_fare_per_duration_diff_normalized',\n",
    "    'predicted_avg_speed_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_per_duration_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff_normalized',\n",
    "    'predicted_additional_fare_diff_normalized',\n",
    "    'predicted_additional_fare_diff_per_additional_fare_normalized',\n",
    "]\n",
    "\n",
    "cat_features = []\n",
    "\n",
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class += model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    models.append(model)\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_pred_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_pred_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':10,\n",
    "    'weights':'uniform'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_pred_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':1,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'auto'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svm_pred_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'predicted_fare_diff_per_fare',\n",
    "    'predicted_fare_diff_per_predicted_fare',\n",
    "    'predicted_fare_diff_per_distance',\n",
    "    'predicted_duraton_diff_per_duraton',\n",
    "    'predicted_duraton_diff_per_predicted_duration',\n",
    "    'predicted_duraton_diff_per_distance',\n",
    "    'predicted_fare_per_duration_diff',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting',\n",
    "    'predicted_meter_waiting_diff_per_distance',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting',\n",
    "    'predicted_meter_waiting_per_duration_diff',\n",
    "    'predicted_meter_waiting_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff',\n",
    "    'predicted_additional_fare_diff_per_additional_fare',\n",
    "    'predicted_addtional_fare_diff_per_distance',\n",
    "    'predicted_meter_waiting_till_pickup_diff_per_meter_waiting_till_pickup',\n",
    "    'predicted_meter_waiting_till_pickup_per_meter_waiting_diff'\n",
    "]\n",
    "\n",
    "cat_features = []\n",
    "\n",
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class += model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    models.append(model)\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_unit_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_unit_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':10,\n",
    "    'weights':'distance'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_unit_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':5,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'auto'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'rbf_svm_unit_diff'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'predicted_fare_diff_per_distance_normalized',\n",
    "    'predicted_fare_diff_normalized',\n",
    "    'predicted_fare_diff_per_fare_normalized',\n",
    "    'predicted_fare_diff_per_predicted_fare_normalized',\n",
    "    'predicted_duraton_diff_per_duraton_normalized',\n",
    "    'predicted_duraton_diff_per_predicted_duration_normalized',\n",
    "    'predicted_fare_per_duration_diff_normalized',\n",
    "    'predicted_avg_speed_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_diff_per_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_diff_per_predicted_meter_waiting_normalized',\n",
    "    'predicted_meter_waiting_per_duration_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_diff_per_predicted_meter_waiting_fare_normalized',\n",
    "    'predicted_meter_waiting_fare_per_meter_waiting_diff_normalized',\n",
    "    'predicted_meter_waiting_fare_per_duration_diff_normalized',\n",
    "    'predicted_additional_fare_diff_normalized',\n",
    "    'predicted_additional_fare_diff_per_additional_fare_normalized',\n",
    "]\n",
    "\n",
    "cat_features = []\n",
    "\n",
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':15,\n",
    "    'weights':'distance'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    org_scores.append(f1_score(y_org,model.predict(train_org.values),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_unit_diff_norm'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':5,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svm_rbf_unit_diff_norm'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'fare_per_distance',\n",
    "    'fare_per_duration',\n",
    "    'avg_speed',\n",
    "    'meter_waiting_per_duration',\n",
    "    'meter_waiting_fare_per_meter_waiting',\n",
    "    'meter_waiting_fare_per_duration',\n",
    "    'addtional_fare_per_fare',\n",
    "    'addtional_fare_per_distance',\n",
    "    'addtional_fare_per_duration',\n",
    "    'fare-additional_fare_per_distance',\n",
    "    'fare-additional_fare_per_duration',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_distance',\n",
    "    'fare-additional_fare-meter_waiting_fare_per_duration',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting',\n",
    "    'meter_waiting_after_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_duration',\n",
    "    'meter_waiting_till_pickup_per_distance',\n",
    "    'meter_waiting_after_pickup_per_distance',\n",
    "    'meter_waiting_till_pickup_per_fare',\n",
    "    'meter_waiting_after_pickup_per_fare',\n",
    "    'meter_waiting_till_pickup_per_meter_waiting_fare',\n",
    "    'meter_waiting_after_pickup_per_meter_waiting_fare',    \n",
    "]\n",
    "\n",
    "cat_features = []\n",
    "\n",
    "train = train_df[features]\n",
    "test = test_df[features]\n",
    "y = train_df['label']\n",
    "for each in cat_features:\n",
    "    train[each] = train[each].values.astype(int)\n",
    "    test[each] = test[each].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'loss_function':'Logloss',\n",
    "    'random_state':0,\n",
    "    'early_stopping_rounds':50,\n",
    "    'eval_metric':'F1',\n",
    "    'border_count':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pool = Pool(data=test_df[features], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "train_class = np.zeros(train_df.shape[0])\n",
    "test_class = np.zeros(test_df.shape[0])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_pool = Pool(data=X_train, label=y_train,cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)    \n",
    "    model = CatBoostClassifier(**catboost_params)\n",
    "    model.fit(X=train_pool, eval_set=test_pool,verbose=10)\n",
    "    train_preds[test_index] = model.predict_proba(test_pool)[:,1]\n",
    "    train_class[test_index] = model.predict(test_pool)\n",
    "    test_preds += model.predict_proba(submission_pool)[:,1]/3\n",
    "    test_class += model.predict(submission_pool)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(test_pool),average='micro'))\n",
    "    models.append(model)\n",
    "test_class = np.where(test_class > 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'catboost_unit'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.05,\n",
    "    'seed':0, \n",
    "    'metric':'f1',\n",
    "    'max_depth':8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_data = lgb.Dataset(X_train,y_train)\n",
    "    valid_data = lgb.Dataset(X_test,y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(lgb_params, train_data,num_boost_round=1000,early_stopping_rounds=50, valid_sets=valid_data,feval=lgb_f1_score, evals_result=evals_result,verbose_eval=False)\n",
    "    \n",
    "    test_preds += model.predict(test) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,np.round(model.predict(X_test)),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lgb_unit'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':10,\n",
    "    'weights':'distance'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = KNeighborsClassifier(**knn_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'knn_unit'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C':10,\n",
    "    'kernel':'rbf',\n",
    "    'random_state':0,    \n",
    "    'probability': False,\n",
    "    'gamma':'auto'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "org_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SVC(**svc_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'svm_unit'\n",
    "model_predictions_train[name] = train_preds\n",
    "model_predictions_test[name] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in model_predictions_test.columns:\n",
    "    model_predictions_train[col] = np.round(model_predictions_train[col]).astype(int)\n",
    "    model_predictions_test[col] = np.round(model_predictions_test[col]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'catboost_unit',\n",
    "    'knn_pred_diff',\n",
    "    'catboost_pred_diff',\n",
    "    'lgb_pred_diff',\n",
    "    'catboost_unit_diff',\n",
    "    'lgb_unit_diff',\n",
    "    'svm_pred_diff',\n",
    "    'svc_linear_anomaly',\n",
    "    'catboost_anomaly',\n",
    "    'rbf_svm_unit_diff',\n",
    "    'rf_anomaly'\n",
    "]\n",
    "cat_cols = [\n",
    "    'knn_pred_diff',\n",
    "    'svm_pred_diff',\n",
    "    'svc_linear_anomaly',\n",
    "    'svc_linear_anomaly',\n",
    "    'rbf_svm_unit_diff',\n",
    "    'rf_anomaly',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in cat_cols:\n",
    "    model_predictions_train[each] = model_predictions_train[each].astype(int)\n",
    "    model_predictions_test[each] = model_predictions_test[each].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model_predictions_train[features]\n",
    "test = model_predictions_test[features]\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_params = {\n",
    "    'penalty':'l2',\n",
    "    'C':1.0,\n",
    "    'solver':'lbfgs',\n",
    "    'max_iter':100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "skf = StratifiedKFold(n_splits=folds)\n",
    "train_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "validation_scores = []\n",
    "models = []\n",
    "for train_index, test_index in skf.split(train, y):\n",
    "    X_train, X_test = train.iloc[train_index,:].values, train.iloc[test_index,:].values\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = LogisticRegression(**logistic_reg_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_preds += model.predict(test.values) / 3\n",
    "    train_preds[test_index] = model.predict(X_test)\n",
    "    validation_scores.append(f1_score(y_test,model.predict(X_test),average='micro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['prediction'] = np.round(test_preds).astype(int)\n",
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
